# Unsupervised Emotion Clustering using Multimodal Features on a Personal Language Corpus

### Abstract

Recent years have seen a spike in research on human affect recognition and emotion detection (ED) in the machine learning community. However, little research has been done to personalize these approaches for individuals looking to explore their own language and emotions. In this paper I perform unsupervised emotion clustering on a personal language corpus using the K-Means algorithm and demonstrate its efficacy using only VAD lexical features and a few emotionally rich acoustic features.

### Introduction

Advances in automatic speech recognition, processing, and transcription technologies have empowered NLP researchers to increasingly draw their corpuses from speech. Though the language of public figures and actors are often chosen for analysis due to the availability of their recordings, the psychological potential of personalized corpuses has largely been ignored. Each individual produces a vast amount of language daily, data that shelters insights into their defense mechanisms, patterns of emotional response, and lives as a whole. In the context of our speech, NLP tools can become therapeutic methods of self-reflection, particularly in the domain of emotional tracking.

One of the principle issues impeding the development of personal emotion recognition tools is the time-consuming labeling process required to train a classification model on an individual's speech, a task made all the more difficult in the light of the often banal and logistical quality of everyday language. The aim of this paper is thus to explore unsupervised clustering methods to perform emotion detection on the lexical and acoustic features of a personal language corpus.


### Related Work

Speech supplies its listener with a rich representation of the communicator's emotional state in both its acoustic and lexical features. But while humans can identify the latent emotional state of an utterance with relative ease, the nuances of a voice's emotional content is rather difficult for a machine to capture. I will begin by reviewing various self-monitoring systems in the psychological domain, then discuss the use of acoustic features to classify mental illnesses, and finally examine mulitmodal approaches to ED.

The mental health crisis attending the pandemic and the prior decade has spawned a remarkable surplus of technology geared towards the recognition, prevention, and alleviation of mental illness. Demand has soared with over 70\% of patients showing interest in using mobile apps to self-monitor their mental health \citep{Torous}. Many have turned towards popular interactive applications like \textit{Headspace} and \textit{Woebot} to get mental health support, approaches which have shown demonstrable positive outcomes for many users \citep{Mani, Fitzpatrick}. Other self-monitoring approaches are journal-like in their approach, requiring patients to log their mood and complete surveys daily to connect them with swift, responsive treatment. This approach was  taken by \citet{Bardrum:2627} in the MONARCA self-assessment system to track early warning signs of bipolar shifts between manic and depressive episodes and by \citet{Rohani:282} in the delivery of behavioral activation therapy to depressed individuals via routine self-assessment on an app.

Self-assessment tools rely heavily on a patients self-recollected and self-perceived behavior for diagnosis and treatment. \citet{Grunerbl} and many others argue that this approach lacks objectivity, while also creating a self-monitoring paradigm that is too heavily predicated on the discipline and willingness of the patient to fill out daily surveys ad infinum. In response, the authors propose a passive monitoring system using acoustic features, location, and acceleration collected via smart-phone sensors to detect manic-depressive swings in bipolar patients, inferring their mental states in an autonomous manner. Despite the authors' innovative approach to patient monitoring using both acoustic features and portable technology (two key ideas in my system), they fail to make use of the semantically-rich lexical features attending the patients' vocal signal.

This focus on acoustic features over lexical content is not isolated to this paper but is pervasive in much psychological research. Clinicians have used vocal features in mental health examinations since \citet{Newman} recognized distinct patterns of vocal response attending various affective disorders, like the “pressured” speech of bipolar patients or “monotone" and “lifeless” speech of depressive patients. More recently, these intuitions have been the basis of various classification algorithms that seek to identify mental illness via the "biological" markers in the voice. These algorithms use low-level descriptors (LLD) of the vocal signal to classify everything from PTSD to Alzheimer's with some success \citep{Marmar, Fraser}. Unfortunately, in pursuit of an "objective" biological signal or metric with which to diagnose mental illness—as if it were as easy as an X-ray detection of a broken bone—computational psychological research has largely ignored the predictive potential of the voice's concurrent lexical information.

ED algorithms proposed by natural language and signal processing researchers often incorporate both acoustic and lexical features into their multimodal models. Lexical feature representations have varied across the years, from the use of key-words and phrase rules \citep{Cowie}, semantic trees \citet{Zhe}, N-Grams \citep{Polzin}, and vector space models \citet{Schuller}. The multimodal model proposed by \citet{Jin} which was later expanded by \citet{Gamage} was particularly successful. In the paper, the authors select the top 500-600 words from each of the four emotion classes represented in the IEMOCAP database using the classic tf-idf weighting method \citep{Busso, Salton}. In addition to these BOW lexical features, the authors propose a new "eVector" 4-dimensional feature to better capture the emotional content of an utterance that weigh words that occur exclusively in certain emotional contexts higher than those that appear uniformly across contexts so as to infuse the model with the emotional salience of words in the lexicon. The attempt to compute emotional salience of particular words hearkens back to the rule-based approach of \citet{Cowie}, but their method is limited to the existence of pre-labeled emotional classes. To uncover the emotional salience of particular words in unsupervised clustering, another approach is required.

Deep-neural networks (DNN) have since replaced many previous feature generation schemes in ED tasks. One example is the multimodal model proposed by \citet{Kim} trained on the same IEMOCAP data. In addition to replacing the BOW features with the semantically rich 300-dimensional word2vec features from the seminal \citet{Mikolov} paper, the authors capture the emotional content of words using a large affective lexicon which maps words onto a 3-D VAD space (valence, arousal, dominance) to model greater intricacies of emotional salience \citep{Warriner}. One potential area of improvement was suggested by \citet{Guo} who applied transformers to better model the interdependence of lexical and acoustic information in an utterance.

Despite the innovative methods of feature encoding that these classification models put forth, there are several issues and assumptions endemic to the IEMOCAP data which brings their results into question: \textbf{(1)} the emotional categories of "happy", "sad", "angry", and "neutral" don't reflect the diversity of emotions that we experience, \textbf{(2)} the situations the actors performed were fabricated from the emotion, rather than emerging from phenomenological experience, creating stereotypical conceptions of said emotion, and \textbf{(3)} the conception of what it means to be "happy", "sad", "angry", or "neutral" are specific to the language/identity of the actors, their communities in Southern California, and the particular days in 2008 in which they were spoken \citep{Busso}. In the attempt to generalize their results and manufacture stable targets for their algorithms, this data does not demonstrate the power of ED algorithms in the real world. For the use case of emotional \textit{self-monitoring} and \textit{self-understanding}, the data is insufficient in size, insufficient in emotional intricacy, and insufficient in its generality to deliver any real benefits to an individual looking to monitor their mind. Part of the contribution of my paper is a personalized approach to data collection that contextualizes emotions in the life of the individual. By hyper-localizing the data to represent the idiosyncratic emotions of a single person, this paper then explores clustering as a possibility to develop semantically meaningful emotional classes in a non-prescriptive manner, combating the reductionist approach that classification algorithms take to this continuous space.

### Methods

#### Data

A core feature of my proposed system is the use of a personal corpus to uncover hyper-local representations of emotional response. This exploration is fueled by data that I have collected over the past 10 months through a lavalier microphone. I envision a Bluetooth device in the future connected to a smartphone to improve ease of access and allow for a cloud-based interface, similar to the system proposed by \citet{Grunerbl}. In total, the corpus is comprised of 160 hours of audio collected over 10 months, with its share of heartbreaks and irritations, outbursts and celebrations. To process this audio into text, I built a simple voice activity detector to isolate voiced segments in the audio and a random forest classification model to extract the time-segments in the audio corresponding to my speech as differentiated from others. These segments are then transcribed using Google's speech-to-text API \citep{https://cloud.google.com/speech-to-text} and matched with its time segment in the original audio for acoustic feature extraction.

#### Feature Extraction

For the acoustic features, I use the \textit{pyAudioAnalysis} library to extract the energy, energy entropy, spectral centroid, spectral rolloff, and zero-crossing rate for each 50 millisecond window of the audio \citep{giannakopoulos2015pyaudioanalysis}. These features are then averaged together across each sentence's time segment to generate sentence-level acoustic feature representations. The acoustic features were chosen based on common sense, their higher relative variances in the data, and their good performance across ED literature \citep{10.3389/fpsyg.2013.00292}. For the lexical features, I will use the emotional salience vectors of \citet{Kim} using the 10,000 common words which were annotated with VAD (valence, arousal, dominance) coordinates by \citet{Warriner}. I average this "e-vector" across the utterance and weigh each word's contribution to the "document" of each audio transcript using the tf-idf weighting scheme of \citet{Salton}. Both the acoustic and the lexical features will be concatenated and then normalized for clustering.


#### K-Means Clustering

In response to the limitations of current self-monitoring paradigms and the limited scope of emotion recognition algorithms, I use a clustering system to group similar emotional responses together. These clusters can be used to train classifiers or be analyzed by themselves. I will cluster the multimodal feature vectors using the k-means algorithm, which minimizes within-cluster variances (dubbed the "inertia") using squared Euclidean distances \citep{scikit-learn}. As the ground truth labels are not known, the optimal number of clusters must be inferred quantitatively using the inertia and the silhouette coefficient, which is a measure that takes into account the mean distance between a sample and all other points in the same class and the mean distance between a sample and all other points in the next nearest cluster \citep{ROUSSEEUW198753}. 

### Evaluation

![Table 1: The silhouette coefficient remains low throughout, leveling off around 0.20 for clusters greater than 10, with a small spike at 10. The point at which the inertia decelerates lies between 7 and 10 clusters.](/Table1.png)

#### Number of Clusters

Emotions are a continuous space in which there are no clear-cut "classes". Any cluster that is thereby formed within this emotional feature space serves only an organizational purpose. This ambiguity taken together with the high density of the data contributes to the difficulty of finding clear-cut classes using standard unsupervised evaluation tools. These lack of clear groupings are evident in Table 1, which shows a silhouette coefficient that is positive but consistently low, indicating the presence of overlapping clusters. The inertia, a measure of internal coherency that represents the minimization of within cluster sums-of-squares, tells us that the the optimal number of clusters lie between 7 and 10, which is confirmed by the silhouette coefficient local maximum at 10. Though the silhouette coefficient is higher at 4 clusters, my aim is to capture more fine-grained emotional representations and thus 10 clusters serves this project better.

#### Qualitative Evaluation

The performance of the clustering cannot be evaluated quantitatively due to the absence of class labels, and as such, we must perform a qualitative evaluation of each of the clusters to determine the their quality. In Table 2, I chose 5 sentences at random from each cluster, listened to their corresponding audio, and whittled away confusing "half-thoughts" and utterances lacking any emotion (logistics) to get three examples per class. I then annotate each sentence with the perceived emotion that I drew from their audio stream. I repeat this process three times in order to validate my assumptions about each class.

Once the logistical, unemotional language had been cleared away, the clustering performed relatively well. Cluster 0 corresponded to many of my long, inspired rants which all had high energy, high lexical affect. The sentences in this cluster tended to be longer, with an average sentence-length of 23 words (much higher than the corpus average of 11). Cluster 1 contained a mess of different emotions, and was among the poorest performing clusters. I believe this cluster is the product of high zero-crossing rates as all of the examples I drew had zero-crossing rates higher than 75\% of the rest of the data. This acoustic feature corresponds to high percussiveness and I will not include it on the next iteration of the K-Means. Most other clusters performed rather well. Cluster 3 was uniformly filled with curse words and slurred speech and represented some of my most distressed moments; cluster 4 was very uniformly disappointed and defeated; cluster 5 was vague, inflective, and uncertain; cluster 6 was more aggressive; cluster 7 was sad, cute, and pitiful; cluster 8 was annoyed; cluster 9 was generally positive and straight forward but as a whole was hard to differentiate from cluster 2. It is important to note that these evaluations are subject to confirmation bias and I hope to expand upon this evaluation in the future with labeled classes.

Although I am using definitive emotions to describe each of these classes, things are a little more ambiguous in reality. If one were to focus purely on the emotionally indicative sentences in each cluster, many of them tended to have similar emotional qualities. But these islands of emotion lie in a sea of logistics and random chit-chat which had little to no emotional valence and were generally evenly split between the clusters. One possibility for future development is to first build a discriminator that decides whether an utterance is emotionally potent or not before clustering.

![Table 2: Certain clusters like cluster 1 had little to no coherence, with a mixture of different sentiments, emotions, and acoustic qualities, others performed very well like clusters 4, 7, and 8.](/Table2.png)

### Limitations & Conclusion 

In this paper I perform unsupervised emotion clustering on a personal language corpus using the K-Means algorithm and demonstrate its efficacy using only VAD lexical features and a few emotionally rich acoustic features. However, this approach suffers from a few limitations. First, it is hard to evaluate models like these, and there is no performance guarantee on any individual's personal data set. Second, the language analyzed is limited to the 10,000 words that exist in the VAD corpus of \citet{Warriner}. This excludes any emotions that I express in my native language of Swedish. Third, due to the different seeding of the K-Means algorithm, the classes do not necessarily converge to the same groupings on each iteration. The lack of deterministic outcomes is also due to the continuous space of emotions, that do not fall into distinct classes. Even humans approaching the same data would have a hard time agreeing on the number or name of classes into which to classify these utterances. More testing must be done in the future using labeled corpuses in order to fully demonstrate this approaches' efficacy.

